{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5acbb1",
   "metadata": {},
   "source": [
    "# EDA+Preprocessing: NLP26 OpenWebText (local download)\n",
    "Load the course split locally (no streaming), cache under dataset/, and inspect a small subset for cleaning decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4fa943",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q datasets tiktoken\n",
    "%pip install -q transformers langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b828876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "CACHE_DIR = \"dataset/hf_cache\"          # existing cache\n",
    "OUT_PATH = pathlib.Path(\"../dataset_final/openwebtext_clean.jsonl\")\n",
    "DATASET_NAME = \"NLP26_OpenWebText\"      # falls back to openwebtext if missing\n",
    "FALLBACK_NAME = \"openwebtext\"\n",
    "SPLIT = \"train\"\n",
    "MAX_TOKENS = 2048                       # truncate to GPT-2 block\n",
    "MIN_CHARS = 50\n",
    "MAX_CHARS = 100_000\n",
    "DROP_CODE = True\n",
    "DROP_NON_EN = True\n",
    "\n",
    "# Dataset percentage to process (40% = ~3.2M docs, 100% = ~8M docs)\n",
    "DATASET_PERCENTAGE = 40.0  # Change this to control how much data to process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc4bd8",
   "metadata": {},
   "source": [
    "### Setting Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "\n",
    "# add repo root to path (handles running from notebooks/)\n",
    "ROOT = pathlib.Path.cwd()\n",
    "if ROOT.name == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "sys.path.insert(0, str(ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b064ca",
   "metadata": {},
   "source": [
    "## Installing the Dataset in Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "CACHE_DIR = \"dataset/hf_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "DATASET_NAME = \"Skylion007/openwebtext\"\n",
    "SPLIT = \"train\"\n",
    "SAMPLE_SIZE = 2000  # adjust if needed\n",
    "\n",
    "ds = None\n",
    "try:\n",
    "    ds = load_dataset(DATASET_NAME, split=SPLIT, cache_dir=CACHE_DIR, streaming=False)\n",
    "except Exception as exc:\n",
    "    print(f\"Falling back to openwebtext because {exc}\")\n",
    "    try:\n",
    "        DATASET_NAME = \"openwebtext\"\n",
    "        ds = load_dataset(DATASET_NAME, split=SPLIT, cache_dir=CACHE_DIR, streaming=False)\n",
    "    except Exception as exc2:\n",
    "        raise RuntimeError(f\"Could not load either dataset: {exc2}\") from exc\n",
    "\n",
    "n = min(SAMPLE_SIZE, len(ds))\n",
    "ds_subset = ds.select(range(n))\n",
    "samples = ds_subset[\"text\"]\n",
    "\n",
    "print(f\"Dataset: {DATASET_NAME} / split={SPLIT}\")\n",
    "print(f\"Loaded sample size: {len(samples)}\")\n",
    "print(\"Example preview (first 500 chars):\")\n",
    "print(samples[0][:500] if samples else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbedcbd",
   "metadata": {},
   "source": [
    "### EDA of RAW Datset\n",
    "\n",
    "We utilize the functions tooken from the service/explore_service.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b23e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.explore_service import load_samples\n",
    "\n",
    "# Load samples with fallback and caching\n",
    "samples, resolved = load_samples(\n",
    "    dataset_name=\"openwebtext\",\n",
    "    split=\"train\",\n",
    "    sample_size=2000,\n",
    "    cache_dir=\"dataset/hf_cache\",\n",
    "    streaming=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfc7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.explore_service import (\n",
    "    length_stats,\n",
    "    token_stats,\n",
    "    flag_counts,\n",
    "    duplicate_stats,\n",
    "    full_eda_report,\n",
    ")\n",
    "\n",
    "# Basic EDA\n",
    "print(length_stats(samples))\n",
    "print(token_stats(samples, tokenizer_name=\"gpt2\", max_samples=2000))\n",
    "print(flag_counts(samples))\n",
    "print(duplicate_stats(samples))\n",
    "print(full_eda_report(samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706ecf2",
   "metadata": {},
   "source": [
    "## Pre-processing of Dataset\n",
    "\n",
    "We use preprocessing_utils functions to apply data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b52bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, re\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "\n",
    "import utils.preprocessing_utils as pu\n",
    "importlib.reload(pu)\n",
    "\n",
    "from utils.preprocessing_utils import (\n",
    "    preprocess_text, \n",
    "    load_tokenizer, \n",
    "    load_test_sentences,\n",
    "    calculate_subset_size\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Optional test sets for leakage removal (put actual paths)\n",
    "TEST_PATHS = [\n",
    "    \"path/to/wikitext_test.txt\",        # replace with real path or leave empty\n",
    "    \"path/to/nlp26_eval.txt\",\n",
    "]\n",
    "\n",
    "# Load tokenizer (gracefully handles missing pkg)\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\", model_max_length=MAX_TOKENS, truncation_side=\"right\")\n",
    "\n",
    "# Load test sentences (can be empty if files not present)\n",
    "test_sents = load_test_sentences(TEST_PATHS)\n",
    "\n",
    "def has_overlap(text: str, test_set: set) -> bool:\n",
    "    if not test_set:\n",
    "        return False\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n",
    "    norm = {re.sub(r\"\\s+\", \" \", s.lower()).strip() for s in sents if s.strip()}\n",
    "    return bool(norm & test_set)\n",
    "\n",
    "# Prepare output\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Stream dataset (downloads first time only; otherwise uses cache)\n",
    "try:\n",
    "    ds = load_dataset(DATASET_NAME, split=SPLIT, cache_dir=CACHE_DIR, streaming=True)\n",
    "    resolved = DATASET_NAME\n",
    "except Exception:\n",
    "    ds = load_dataset(FALLBACK_NAME, split=SPLIT, cache_dir=CACHE_DIR, streaming=True)\n",
    "    resolved = FALLBACK_NAME\n",
    "\n",
    "# Calculate how many documents to process based on percentage\n",
    "# OpenWebText has ~8,013,769 documents total\n",
    "TOTAL_DATASET_SIZE = 8_013_769\n",
    "MAX_DOCS = calculate_subset_size(TOTAL_DATASET_SIZE, DATASET_PERCENTAGE)\n",
    "\n",
    "print(f\"Processing {DATASET_PERCENTAGE}% of dataset\")\n",
    "print(f\"Total documents to process: {MAX_DOCS:,} out of {TOTAL_DATASET_SIZE:,}\")\n",
    "\n",
    "total = kept = filtered_overlap = filtered_clean = 0\n",
    "\n",
    "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in tqdm(ds, desc=f\"Processing {resolved}\"):\n",
    "        if total >= MAX_DOCS:\n",
    "            break\n",
    "        total += 1\n",
    "        raw = ex.get(\"text\", \"\")\n",
    "        cleaned = preprocess_text(\n",
    "            raw,\n",
    "            tokenizer=tok if not isinstance(tok, Exception) else None,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            drop_code=DROP_CODE,\n",
    "            drop_non_english=DROP_NON_EN,\n",
    "            url_placeholder=\"<url>\",\n",
    "            min_chars=MIN_CHARS,\n",
    "            max_chars=MAX_CHARS,\n",
    "        )\n",
    "        if not cleaned:\n",
    "            filtered_clean += 1\n",
    "            continue\n",
    "        if has_overlap(cleaned, test_sents):\n",
    "            filtered_overlap += 1\n",
    "            continue\n",
    "        json.dump({\"text\": cleaned}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "        kept += 1\n",
    "\n",
    "print(f\"Dataset: {resolved}\")\n",
    "print(f\"Total seen: {total}\")\n",
    "print(f\"Kept: {kept}\")\n",
    "print(f\"Filtered (clean rules): {filtered_clean}\")\n",
    "print(f\"Filtered (overlap): {filtered_overlap}\")\n",
    "print(f\"Output: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da212486",
   "metadata": {},
   "source": [
    "### EDA on cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc9e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ecd165f05047a0b118be5af2a466df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 5000, 'min': 296, 'max': 11343, 'avg': 3927.7208, 'short_lt_200': 0, 'short_pct': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1146 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 5000, 'min': 62, 'max': 2048, 'avg': 854.8502, 'over_2048': 0, 'over_2048_pct': 0.0}\n",
      "{'html': 317, 'code': 0, 'non_en': 0, 'url': 1, 'ctrl': 5000, 'email': 0, 'phone': 0}\n",
      "{'total': 5000, 'unique': 5000, 'dupes': 0, 'dupe_pct': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from utils.explore_utils import (length_stats, token_stats, flag_counts, duplicate_stats,)\n",
    "\n",
    "CLEAN_JSONL = str(OUT_PATH)\n",
    "EDA_SAMPLE = 5000 \n",
    "\n",
    "eda_ds = load_dataset(\"json\", data_files=CLEAN_JSONL, split=\"train[:{}]\".format(EDA_SAMPLE)) # load a sample for EDA\n",
    "texts = eda_ds[\"text\"] # extract text field for analysis\n",
    "\n",
    "print(length_stats(texts)) #\n",
    "print(token_stats(texts, tokenizer_name=\"gpt2\", max_samples=EDA_SAMPLE))\n",
    "print(flag_counts(texts))\n",
    "print(duplicate_stats(texts))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}



