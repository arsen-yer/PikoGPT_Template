{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5b3687",
   "metadata": {},
   "source": [
    "# Theory in Code: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbc5e6",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a118bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445788de",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "example_text_to_tokenize = \"This is an example sentence to be tokenized.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04cd1f",
   "metadata": {},
   "source": [
    "## Basic GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c93b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "## Loads the same tokenizer as above\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "tokens = tokenizer(example_text_to_tokenize)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode tokens back to text\n",
    "decoded_text = tokenizer.decode(tokens[\"input_ids\"])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f10a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all special tokens\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29116648",
   "metadata": {},
   "source": [
    "GPT-2 has only one true special token: <|endoftext|>\n",
    "\n",
    "\n",
    "No padding -> you can use eos_token for padding or add a custom pad_token as special token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Vocab size from the tokenizer\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge cases\n",
    "# edge case 1: empty string\n",
    "print(tokenizer(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714be78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge case 2: string with only spaces\n",
    "print(tokenizer(\"     \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13282869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge case 3: emojis and special characters\n",
    "print(\"ðŸ˜Š:\" ,tokenizer(\"ðŸ˜Š\"))\n",
    "print(\"ä¸¤:\", tokenizer(\"ä¸¤\"))\n",
    "print(\"NewLine: \", tokenizer(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge case 4: Supercalifragilisticexpialidocious\n",
    "print(tokenizer(\"Supercalifragilisticexpialidocious\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge case 5: DonauÂ­dampfschifffahrtsÂ­elektrizitÃ¤tenÂ­hauptbetriebswerkÂ­bauunterbeamtenÂ­gesellschaft\n",
    "print(tokenizer(\"DonauÂ­dampfschifffahrtsÂ­elektrizitÃ¤tenÂ­hauptbetriebswerkÂ­bauunterbeamtenÂ­gesellschaft\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e03c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge case 6: special token\n",
    "print(tokenizer(tokenizer.eos_token))\n",
    "print(tokenizer(tokenizer.bos_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8063bc",
   "metadata": {},
   "source": [
    "### Compare to other Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 tokenizer\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer(example_text_to_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "bert_tokenizer = transformers.BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokens = bert_tokenizer(example_text_to_tokenize)\n",
    "print(bert_tokenizer.vocab_size)\n",
    "print(bert_tokenizer.special_tokens_map)\n",
    "print(bert_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1796a3c",
   "metadata": {},
   "source": [
    "OpenAI Tokenizer: [OpenAI Platform Tokenizer Playground](https://platform.openai.com/tokenizer)\n",
    "\n",
    "*A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7614c5",
   "metadata": {},
   "source": [
    "## Example: NanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb8f42",
   "metadata": {},
   "source": [
    "The full code from nanoGPT: [nanoGPT/dat/openwebtext/prepare.py](https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer through tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_proc_load_dataset = 8 # number of processes to use for loading the dataset\n",
    "# dataset = load_dataset(\"openwebtext\", split=\"train[:5]\", num_proc=num_proc_load_dataset)\n",
    "\n",
    "## lighter variant with streaming, which does not load the whole dataset into memory\n",
    "dataset = load_dataset(\"openwebtext\", split=\"train\", streaming=True)\n",
    "# take only the first\n",
    "dataset = list(dataset.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we just used the steaming variant, we need to convert the list back to a Dataset object\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(dataset)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split with seed\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.05, seed=SEED, shuffle=True)\n",
    "\n",
    "# rename test to validation\n",
    "split_dataset['val'] = split_dataset.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nanoGPT function\n",
    "def process(example):\n",
    "        ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "        ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n",
    "        # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n",
    "        out = {'ids': ids, 'len': len(ids)}\n",
    "        return out\n",
    "\n",
    "# tokenize the dataset\n",
    "tokenized = split_dataset.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623d44d",
   "metadata": {},
   "source": [
    "Map-Function -> Smart way of a for each loop\n",
    "\n",
    "```\n",
    "for each example in dataset:\n",
    "    new_example = process(example)\n",
    "```\n",
    "\n",
    "**Why map?**\n",
    "- Preserves the dataset structure (keeps column names, features etc.)\n",
    "- alows parallelization (num_proc)\n",
    "- memory efficient\n",
    "- supports batching (Better GPU utilization, Vectorized tokenization)\n",
    "- designed for large-scale ML\n",
    "\n",
    "\n",
    "**use map when working with datasets**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6844b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first rows of train and validation split\n",
    "print(tokenized['train'][0])\n",
    "print(tokenized['val'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6388d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use map to get the text back from the tokenized dataset\n",
    "def decode(example):\n",
    "    text = enc.decode(example['ids'])\n",
    "    return {'text': text}\n",
    "\n",
    "decoded = tokenized.map(\n",
    "    decode,\n",
    "    remove_columns=['ids', 'len'],\n",
    "    desc=\"decoding the splits\",\n",
    "    num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fa10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoded['train'][0])\n",
    "print(decoded['val'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e87a60",
   "metadata": {},
   "source": [
    "## Structure Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6b663",
   "metadata": {},
   "source": [
    "function head out of: pikogpt/src/data/preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "def tokenize_documents(\n",
    "    texts: list[str],\n",
    "    tokenizer: GPT2TokenizerFast,\n",
    "    batch_size: int = 1000,\n",
    "    show_progress: bool = True\n",
    "    ) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Tokenize documents with EOT token appended.\n",
    "\n",
    "    Args:\n",
    "        texts: List of text documents\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        batch_size: Number of documents to tokenize at once\n",
    "        show_progress: Whether to show progress bar\n",
    "\n",
    "    Returns:\n",
    "        List of token ID lists (one per document)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42c50a",
   "metadata": {},
   "source": [
    "## Project Stages Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e9f64",
   "metadata": {},
   "source": [
    "- Example Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
