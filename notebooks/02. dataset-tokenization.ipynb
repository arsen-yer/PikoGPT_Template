{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5b3687",
   "metadata": {},
   "source": [
    "# PikoGPT Pretraining Pipeline\n",
    "Training a GPT model on OpenWebText dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbc5e6",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a118bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Setup project paths\n",
    "ROOT = pathlib.Path.cwd()\n",
    "if ROOT.name == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e5430",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "class Config:\n",
    "    # Data\n",
    "    data_path = ROOT / \"data\" / \"dataset_final\" / \"openwebtext_clean.jsonl\"\n",
    "    \n",
    "    # Model Architecture\n",
    "    vocab_size = 50257  # GPT-2 vocab size\n",
    "    n_layer = 6         # Number of transformer blocks\n",
    "    n_head = 6          # Number of attention heads\n",
    "    n_embd = 384        # Embedding dimension\n",
    "    block_size = 1024   # Maximum sequence length\n",
    "    dropout = 0.1       # Dropout rate\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 8      # Batch size (adjust based on GPU memory)\n",
    "    max_iters = 5000    # Total training iterations\n",
    "    learning_rate = 3e-4\n",
    "    weight_decay = 0.1\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.95\n",
    "    grad_clip = 1.0     # Gradient clipping\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_interval = 500  # Evaluate every N iterations\n",
    "    eval_iters = 100     # Number of iterations for evaluation\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_dir = ROOT / \"runs\" / \"checkpoints\"\n",
    "    save_interval = 1000  # Save checkpoint every N iterations\n",
    "    \n",
    "    # System\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed = 42\n",
    "\n",
    "config = Config()\n",
    "config.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Data path: {config.data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04cd1f",
   "metadata": {},
   "source": [
    "## 3. Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c93b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT-2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token (GPT-2 doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "print(f\"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenization\n",
    "example_text = \"This is an example sentence for tokenization.\"\n",
    "tokens = tokenizer(example_text, return_tensors=\"pt\")\n",
    "print(f\"\\nOriginal text: {example_text}\")\n",
    "print(f\"Token IDs: {tokens['input_ids']}\")\n",
    "print(f\"Decoded: {tokenizer.decode(tokens['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4991c9",
   "metadata": {},
   "source": [
    "## 4. Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90069db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for loading and tokenizing JSONL text data\"\"\"\n",
    "    \n",
    "    def __init__(self, jsonl_path, tokenizer, block_size, max_samples=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.examples = []\n",
    "        \n",
    "        print(f\"Loading data from {jsonl_path}...\")\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(tqdm(f)):\n",
    "                if max_samples and i >= max_samples:\n",
    "                    break\n",
    "                data = json.loads(line)\n",
    "                text = data.get('text', '')\n",
    "                if text.strip():  # Skip empty texts\n",
    "                    self.examples.append(text)\n",
    "        \n",
    "        print(f\"Loaded {len(self.examples)} documents\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.examples[idx]\n",
    "        \n",
    "        # Tokenize and truncate to block_size\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.block_size + 1,  # +1 for labels\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        \n",
    "        # Create inputs and labels (shifted by 1 for next-token prediction)\n",
    "        x = input_ids[:-1]\n",
    "        y = input_ids[1:]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Load dataset (use max_samples for testing, remove for full training)\n",
    "train_dataset = TextDataset(\n",
    "    config.data_path,\n",
    "    tokenizer,\n",
    "    config.block_size,\n",
    "    max_samples=1000  # Remove this for full dataset\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(train_dataset)}\")\n",
    "print(f\"Block size: {config.block_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 on Windows to avoid multiprocessing issues\n",
    "    pin_memory=True if config.device == \"cuda\" else False\n",
    ")\n",
    "\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "\n",
    "# Test batch loading\n",
    "x_sample, y_sample = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  Input (x): {x_sample.shape}\")  # [batch_size, block_size]\n",
    "print(f\"  Labels (y): {y_sample.shape}\")  # [batch_size, block_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6782fbd0",
   "metadata": {},
   "source": [
    "## 5. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        # Key, query, value projections\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        \n",
    "        # Causal mask  \n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch, sequence length, embedding dim\n",
    "        \n",
    "        # Calculate query, key, values\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        \n",
    "        # Re-assemble head outputs\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT Language Model\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),  # Token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),  # Position embeddings\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        print(f\"Model initialized with {self.get_num_params()/1e6:.2f}M parameters\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Sequence length {t} exceeds block size {self.config.block_size}\"\n",
    "        \n",
    "        # Forward pass\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        # Calculate loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "# Initialize model\n",
    "model = GPT(config)\n",
    "model = model.to(config.device)\n",
    "print(f\"Model moved to {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6a547",
   "metadata": {},
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13282869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(config.beta1, config.beta2),\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Weight decay: {config.weight_decay}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, data_loader, max_iters):\n",
    "    \"\"\"Estimate average loss on dataset\"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        if i >= max_iters:\n",
    "            break\n",
    "        x, y = x.to(config.device), y.to(config.device)\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    model.train()\n",
    "    return np.mean(losses)\n",
    "\n",
    "def save_checkpoint(model, optimizer, iteration, loss, path):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    torch.save({\n",
    "        'iteration': iteration,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'config': config.__dict__\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    \"\"\"Load model checkpoint\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=config.device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['iteration'], checkpoint['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29021078",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e03c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "train_losses = []\n",
    "iterations = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "iter_num = 0\n",
    "running_loss = 0.0\n",
    "progress_bar = tqdm(total=config.max_iters, desc=\"Training\")\n",
    "\n",
    "while iter_num < config.max_iters:\n",
    "    for x, y in train_loader:\n",
    "        # Move data to device\n",
    "        x, y = x.to(config.device), y.to(config.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(x, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Evaluation\n",
    "        if iter_num % config.eval_interval == 0 and iter_num > 0:\n",
    "            avg_loss = running_loss / config.eval_interval\n",
    "            train_losses.append(avg_loss)\n",
    "            iterations.append(iter_num)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}', \n",
    "                'iter': iter_num\n",
    "            })\n",
    "            \n",
    "            running_loss = 0.0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if iter_num % config.save_interval == 0 and iter_num > 0:\n",
    "            checkpoint_path = config.checkpoint_dir / f\"checkpoint_iter_{iter_num}.pt\"\n",
    "            save_checkpoint(model, optimizer, iter_num, loss.item(), checkpoint_path)\n",
    "        \n",
    "        iter_num += 1\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if iter_num >= config.max_iters:\n",
    "            break\n",
    "\n",
    "progress_bar.close()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save final model\n",
    "final_checkpoint_path = config.checkpoint_dir / \"final_model.pt\"\n",
    "save_checkpoint(model, optimizer, iter_num, loss.item(), final_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8063bc",
   "metadata": {},
   "source": [
    "## 8. Text Generation & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8, top_k=50):\n",
    "    \"\"\"Generate text from a prompt\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt').to(config.device)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop tokens if sequence is too long\n",
    "        tokens_cond = tokens if tokens.size(1) <= config.block_size else tokens[:, -config.block_size:]\n",
    "        \n",
    "        # Get predictions\n",
    "        logits, _ = model(tokens_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Top-k sampling\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        \n",
    "        # Stop if EOS token is generated\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Decode and return\n",
    "    return tokenizer.decode(tokens[0].tolist())\n",
    "\n",
    "# Test text generation\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In a world where\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Generated Text Samples\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_new_tokens=50)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(train_losses) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(iterations, train_losses, label='Training Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough iterations for loss plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1796a3c",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "**To continue training:**\n",
    "- Remove `max_samples=1000` limit in dataset loading (line with TextDataset)\n",
    "- Increase `max_iters` to 10000+ for better results\n",
    "- Adjust `batch_size` based on your GPU memory\n",
    "\n",
    "**To load a checkpoint:**\n",
    "```python\n",
    "checkpoint_path = config.checkpoint_dir / \"checkpoint_iter_1000.pt\"\n",
    "iter_num, loss = load_checkpoint(model, optimizer, checkpoint_path)\n",
    "print(f\"Loaded checkpoint from iteration {iter_num} with loss {loss:.4f}\")\n",
    "```\n",
    "\n",
    "**Improvements:**\n",
    "- Learning rate scheduling (cosine decay, warmup)\n",
    "- Validation set evaluation\n",
    "- Gradient accumulation for larger effective batch size\n",
    "- Mixed precision training (torch.cuda.amp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
